% This is "sig-alternate.tex" V2.0 May 2012
% This file should be compiled with V2.5 of "sig-alternate.cls" May 2012
%
% This example file demonstrates the use of the 'sig-alternate.cls'
% V2.5 LaTeX2e document class file. 

\documentclass{sig-alternate}
\usepackage{color}
\usepackage[colorlinks,citecolor=blue]{hyperref}
\usepackage[utf8]{inputenc}
\begin{document}

\conferenceinfo{Information Retrieval}{2016 DIKU, Denmark}
\title{Result re-ranking}
\numberofauthors{1} 
\author{
\alignauthor 
 dpj482 - Christian Edsberg MÃ¸llgaard
}
\maketitle

\begin{abstract}
In this paper we will peform reranking of search results based on an intra-document methodology. This methodology does not only take term frequency into account, but also tries to use the relative importance of terms in context to individual  documents. The methodology used is proposed by Woo et. al\cite{cummins2011improved}. Three datasets are used to evaluate the reranking against the original ranking. Here i find no evidence of an improvement and in most cases get worse results than before.
\end{abstract}
\section{Introduction}
With the growing amount of documents on the web, precise and fast searches becomes even more important. In many traditional models, they make use of simple statistics to rank documents relevant to searches. I have tried to re-rank a traditional model to provide better precision.

Other studies\cite{woo2010achieving} have already shown this to be effective, and I want to further explore how this improves searches of on other data sets.

The reason for doing this is, that high precision searches come at a high computational cost. So a simple and fast method is used to get the top results. The top rankings then get re-ranked based on two other models, which we introduce later on. 

The model we propose, is based on tf-idf. It determines rank based on the amount of occurrences of a term in a document, and thus provides a query a higher chance of getting ranked the highest. This very simple model has been effective in many other experiments. The tf-idf does however not consider how the document is build. For instance a query $x$ would rank two documents $a b x x c$ and $a x b x c$ the same. However the second document would have a higher chance of the document being about $x$, as the mentions of $x$ is spread out through the document, and not just mentioned several times in the same sentence, which could be the explanation in document one.

The two models we need, needs to satisfy the following hypothesises:
\begin{itemize}
\item When terms of a query are ranked relatively higher for a document, there is a chance that the query is highly relevant for the document
\item If terms of a query are ranked close to each other, they probably describe the contents of the document, which implies higher relevance.
\end{itemize}
\subsection{Other experiments that could influence my result}

The great thing about this method is, that it works very well alongside other models. It is possible to combine this method with many other models. As an example you can look at the research found here\cite{review1} where they explore how to optimize the tf-idf part. These two papers explore how to tune the weights used when calculating tf-idf. Implementing this, you would increase the effectiveness of the first ranking, which could then be improved upon using our re-ranking method. 

According to the articel\cite{review1} they gained about 10\% rank precision by addressing the problem, that most models use a static weights when calculating their ranks. These static weights are great for specific sized documents and such, but they grow more imprecise the longer the document is from the ideal document size. To handle this they implemented a weight scheme, where they have a weight that is great for short documents and a weight for long documents. They then combine these two weights based on the documents, so they get a more balanced weight regardless of document size.

Another article\cite{review3} makes use of weighing terms differently depending on how the affect the query. They show that not every term in the query should be weighted the same. The example they use in the article is:\\
Consider the following queries with the same term \textbf{effect}.
\begin{itemize}
\item How does Doppler ultrasound take advantage of the
Doppler effect to create a moving image of the inside
of the body?
\item Effect of temperature on measurement of alkaline phosphatase
activity
\end{itemize}
Effect is the core term when a human reader interprets the query, but the computer will usually weight it equally with all the others. 

They have thus implemented a method to check if a term should be weighed higher or lower than the others. This have given them -1\% to 26\% better results, and according to them does not increase the computational cost alot, which indicates that this could could also be implemented side by side with our term placement re-ranking.

\subsection{Another take at re-ranking}
Another article\cite{review2} have taken another look at re-ranking and figured another way to use it. They have implemented a system where users can personally reconfigure the term weights of searches. This way they can themselves influence their own search results based on the current result. It is mostly and experimental system, but it can provide information about how users really want their searches weighed. 

What they do is that they initially just pass the query and provides default answers. The user is then able to manually handle terms, so they get just the results they are looking for, without trying to redo the search.

 \section{Data sets and queries}
This article makes use of three publicly available data sets with documents, queries and qrels (optiman results for all queries). 

For the documents data sets it looks like this:
\begin{table}[h!]
\centering
\caption{document data sets}
\begin{tabular}{|l|l|l|l|}
\hline
Name           & laTimes & fbis   & ft     \\ \hline
document count & 131896  & 130471 & 210158 \\ \hline
average length & 502     & 504    & 399    \\ \hline
minimum length & 2       & 12     & 11     \\ \hline
maximum        & 24125   & 143175 & 26145  \\ \hline
\end{tabular}
\end{table}

for the queries I use it looks like this:

\begin{table}[h!]
\centering
\caption{queries data set}
\begin{tabular}{|l|l|l|}
\hline
 & query count & average length  \\ \hline
queries & 150 & 19.54     \\ \hline
%average length & 19.54   \\ 
\end{tabular}
\end{table}

These are the data sets used for the experiments in this assignment.

\section{experimental Setup}
The experiment is carried out using various tools, where the most important are: Indri, Python 2.7, c++ and \\TREC\_EVAL. To replicate the experiments follow the guide in the appendix.

\section{experiments}
As a baseline for the experiment, LM with dirichlet smoothening was used. 20 or 50 documents are retrieved for each query, and to tune the dirichlet smoothening, $\mu$ is set to ${1000,1500,...,3000}$ to see when it peforms best for each dataset and query return count. The tuning is done using $P_5$, and the results are shown in table 3. The tuning of $\mu$ turned out to be identical for both query count 20 and 50. The best $\mu$ is reported in bold.

\begin{table}[h!]
\centering
\caption{$P@5$ values used to tune the experiment}
\begin{tabular}{|l|l|l|l|l|l|}
\hline
 $\mu$ & 1000 & 1500 & 2000 & 2500 & 3000  \\ \hline
LATIMES & 0.2907 & 0.2920 & 0.2960 & \textbf{0.2973} & 0.2947 \\ \hline
FT & 0.3221 & \textbf{0.3248} & 0.3195 & 0.3221 & 0.3235  \\ \hline
FBIS & 0.2581 & \textbf{0.2608} & 0.2554 & 0.2554 & 0.2486 \\  \hline
%average length & 19.54   \\ 
\end{tabular}
\end{table}

The next step is to calculate the two heuristics used in the reranking using the $\mu$ values calculated before. The heuristics called R1 and R2, are then used to rerank the querys. The result is then reported using TREC\_EVAL with in several different situations. These results are shown in table 4 for query count 20 and in table 5 for query count 50.

\begin{table}[h!]
\centering
\begin{tabular}{|l|l|l|l|l|l|}
\hline
name & type  & lm & +R1 & +R2 & +R1+R2 \\ \hline \hline
la & P\_5 & 0.5137 & 0.4104 & 0.4372 & 0.4402 \\ \hline 
la & MMR & 0.2947 & 0.2333 & 0.2427 & 0.2427 \\ \hline
la & NDCG@5 & 0.3195 & 0.2414 & 0.2544 & 0.2547 \\ \hline \hline
ft & P\_5 & 0.5422 & 0.4386 & 0.4198 & 0.4322 \\ \hline 
ft & MMR & 0.3235 & 0.2483 & 0.2483 & 0.2564 \\ \hline
ft & NDCG@5 & 0.3497 & 0.2657 & 0.2586 & 0.2672 \\ \hline \hline
fbis & P\_5 & 0.4011 & 0.3374 & 0.3127 & 0.3307 \\ \hline 
fbis & MMR & 0.2486 & 0.2095 & 0.2068 & 0.2108 \\ \hline
fbis & NDCG@5 & 0.2664 & 0.2148 & 0.2085 & 0.2134 \\ \hline
\end{tabular}
\caption{Reranked results for count 20}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{|l|l|l|l|l|l|}
\hline
name & type  & lm & +R1 & +R2 & +R1+R2 \\ \hline \hline
la & P\_5 & 0.5143 & 0.3676 & 0.3872 & 0.4084 \\ \hline 
la & MMR & 0.2947 & 0.1867 & 0.1893 & 0.1933 \\ \hline
la & NDCG@5 & 0.3195 & 0.2014 & 0.2068 & 0.2153 \\ \hline \hline
ft & P\_5 & 0.5430 & 0.4031 & 0.3935 & 0.3900 \\ \hline 
ft & MMR & 0.3235 & 0.2255 & 0.2295 & 0.2255 \\ \hline
ft & NDCG@5 & 0.3497 & 0.2396 & 0.2399 & 0.2349 \\ \hline \hline
fbis & P\_5 & 0.4026 & 0.2810 & 0.2893 & 0.2906 \\ \hline 
fbis & MMR & 0.2486 & 0.1730 & 0.1703 & 0.1716 \\ \hline
fbis & NDCG@5 & 0.2664 & 0.1742 & 0.1765 & 0.1752 \\ \hline
\end{tabular}
\caption{Reranked results for count 50}
\end{table}






%% old thingy
\begin{abstract}
Write your abstract here. Your abstract should \emph{concisely} say (i) \emph{why} the topic is interesting, (ii) \emph{what} you do in your study, (iii) \emph{how} you did your study and (iv) \emph{what} the results were
\end{abstract}

\section{Introduction}
Write your introduction here. Get inspiration on how to structure and formulate an introduction from the studies you review. Make sure you describe what document re-ranking is and why it is useful.

Several studies on document re-ranking have been published before and after \cite{woo2010achieving}. In your introduction, give an overview of \textbf{\emph{at least three}} studies of re-ranking documents published by the ACM between 2003 and 2016. You may find these studies by searching e.g.\ the ACM digital library (\url{http://dl.acm.org}) or Google Scholar. Your literature review of the $3+$ papers must (i) describe the method proposed in the paper, how the method was evaluated and what the results were. Furthermore, it should be clear how each paper differ from the other paper you review. \textbf{Remember: the literature review is meant to help the reader understand where there is a gap in the existing research that you can fill}. Therefore, select papers that are as close as possible to \cite{woo2010achieving}.

You \emph{must} cite your sources when/if you use a specific phrasing. Failure to do so will be considered plagiarism.

\section{Data sets \& Queries}
List in a table, for each data set, the following characteristics:
\begin{enumerate}
\item Name
\item Number of documents
\item Average document length
\item Minimum document length
\item Maximum document length
\end{enumerate}

Furthermore, list the number of queries and the average query length for the superset of the queries.

\section{Experimental setup}
Describe your experimental setup. Do this similarly to how you did in your original projects.

\section{Experiments}
For indexing, retrieval and evaluation use \textsc{Indri} and use \textsc{trec\_eval}. Index the data sets using the Krovetz stemmer and stop word removal using the list \url{http://ir.dcs.gla.ac.uk/resources/linguistic_utils/stop_words}

Use the title field of the queries (preprocessing of the queries is required to extract the title) and retrieve the top-$k = 20,50$ documents using a query-likelihood language model with Dirichlet smoothing for baseline. Tune your retrieval by setting $\mu = \{1000,1500,2000,2500,3000\}$, and optimise for precision at 5 (P@5). Report this for each data set. Report the best P@5 you obtain. 

Use both $R_1$ and $R_2$ for experiments and re-rank the documents of the initial baseline for both top-$k = 20,50$ documents as specified in \cite{woo2010achieving} using (i) $LM+R_1$, (ii) $LM+R_2$ and (iii) $LM+R_1+R_2$ and evaluate the results using mean reciprocal rank (MRR), precision at 1 (P@1), P@5 and normalised discounted cumulative gain at 5 (NDCG@5). What do you observe? Which heuristic or combination of heuristics performs the best? Present your results in a table similar to Table 1 in \cite{woo2010achieving}. If you have time, can you come up with a heuristic that performs better?

\section{Conclusion}
Write your conclusion here
\bibliographystyle{abbrv}
\bibliography{sigproc}  % sigproc.bib is the name of the 
\end{document}
